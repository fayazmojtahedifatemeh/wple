take a look at this and improve , overhaul and upgrade the current : This guide is structured for an AI to understand the technical requirements and implementation steps necessary to build a comprehensive, multi-site personal wishlist and price tracker application, drawing on the architecture, tools, and challenges detailed in the sources and our analysis of 14 e-commerce sites.

---

## AI Implementation Guide: Wishlist and Price Tracker Application

The objective is to create an application that performs continuous, reliable monitoring of product prices and details (name, images, colors, availability) across diverse e-commerce platforms (Amazon, Zara, H&M, etc.), maintains historical data (3 months, all time), and alerts the user upon price decreases or restocks.

### Phase 1: Establishing the Core Architecture

The application requires a robust, server-side architecture capable of handling long-running background tasks and persistent data storage.

1.  **Select the Framework:** Choose a framework that supports both frontend display and powerful backend data management.
    *   **Option A: Django (Python):** Excellent for database modeling and integration with background processes.
    *   **Option B: Next.js (JavaScript):** Utilizes server actions and API routes, integrating scraping logic directly into the stack.
2.  **Define Persistence Layer (Database):** Use a NoSQL database (like **MongoDB** with Mongoose) or SQL (with Django models) to store complex, nested data structures.
    *   Implement database connectivity on every server action or task call to ensure the connection is maintained.
3.  **Integrate Background Task Management:** Continuous price checking requires execution independent of user activity.
    *   **Django Requirement:** Integrate **Celery** (the background process manager) with **Redis** (the data store/broker). This allows Python functions (the scraper) to run on a set schedule (`beat` schedule).
    *   **Next.js Requirement:** Configure **API Routes** to be executed by external **Cron jobs**.

### Phase 2: Data Acquisition and Scraping Logic

The scraper must be flexible, capable of handling static HTML, structured JSON, and content loaded dynamically by JavaScript.

#### 2.1. Tool Selection Based on Site Complexity

The scraping method must adapt to the target website:

*   **Static HTML/JSON Sites (H&M, The Outnet, Basic Prices):** Use basic parsers for efficiency.
    *   **Python:** **Beautiful Soup (BS4)** combined with `urllib.request`.
    *   **Node.js:** **Cheerio** paired with `axios`.
    *   *Strategy:* Prioritize scraping structured data embedded in the HTML, such as the JSON found in H&M's product schema, as this is typically more stable than complex CSS selectors.
*   **Dynamic/JavaScript-Heavy Sites (Zara, J.Crew Sizes):** Use a programmable browser.
    *   **Python:** **Selenium**.
    *   **Node.js:** **Puppeteer**.
    *   *Strategy:* The script must launch a headless browser instance, navigate to the URL, wait for the JavaScript content (like the size list dropdown for J.Crew or Zara) to load, and then parse the rendered HTML.

#### 2.2. Multi-Site Data Extraction Strategy

The application must maintain a unique, specialized scraping function for *each* targeted retailer, as class names and data locations are never standardized (as shown by the 14 site blueprints).

1.  **Identify Target Selectors:** Use browser developer tools to find unique HTML markers (IDs, classes, or data attributes like `[data-qa]`, `[data-testid]`) for each required data point.
    *   **Example (Amazon):** Find the name using ID `productTitle`; price is often spread across nested spans like `.a-price-whole`.
    *   **Example (Variants):** Determine size availability by checking HTML attributes such as `aria-disabled="true"` (Coach Outlet) or specific class names (YOOX's `SizePicker_disabled__ma4Lp`).
2.  **Extract Data and Reconstruct Product Metadata:**
    *   **Retrieve Text:** Use the parsing tool's `.text` or `get text` methods.
    *   **Price Cleaning:** Crucially, remove currency symbols and commas from the price string, then convert the result to a usable **floating-point number** for calculations and comparison.
    *   **Extract Images:** Images often require parsing custom HTML attributes (`data-src`, `data-zoom`) or parsing the `srcset` string to get the highest resolution URL.

### Phase 3: Data Persistence and Historical Tracking

The extracted data must be structured and stored for long-term price analysis.

1.  **Define Database Models:** Create at least two primary models:
    *   **Product:** Stores permanent product details (Title, Brand, Image URL, ASIN/Unique ID, Current Price, URL, `is_active`).
    *   **ProductScrapeEvent:** Stores a timestamped record of every check.
2.  **Implement Historical Tracking:** Use a **`priceHistory` array** within the database model.
    *   Each successful price check must append the price and the date to this array.
    *   This array serves as the source for generating 3-month and **all-time price history** records, and for calculating metrics such as **Lowest Price**, **Highest Price**, and **Average Price**.
3.  **Unique Identification:** Use the product's unique identifier (e.g., the **ASIN** for Amazon products) as a key element to reliably update or find the correct product in the database.

### Phase 4: Automation and User Alerting

The application must automatically check prices and trigger notifications when criteria are met.

1.  **Schedule the Scrape Task:** Use the configured scheduler (**Celery Beat** or **Cron job**) to call the main scraping routine periodically (e.g., daily or every 12 hours).
    *   The task iterates through all active products in the database (`Product.objects.filter(active=True)`) and calls the dedicated scraping function for each one.
2.  **Price Decrease Detection:** Within the scheduled task, apply the core price logic:
    *   Check if the **latest recorded price** (index -1 in the `priceHistory` array) is **less than its previous element** (index -2).
    *   Calculate the exact price decrease amount.
    *   Optionally, check if a **threshold** is met (e.g., price dropped by 40%).
3.  **Email Notification System:** Use a reliable email library (`SMTP Lib` or `Node Mailer`) to send alerts.
    *   Configure a **dedicated, secondary email account** and enable necessary permissions ("less secure apps").
    *   If a price decrease or restock is detected, call the `send email` function, passing the product details and the specific notification type (e.g., welcome, change of stock, lowest price).

### Phase 5: Ensuring Reliability and Scaling (The Constraint of "Completely Free")

For continuous, reliable operation, the AI must address sophisticated anti-scraping measures employed by major retailers.

1.  **Overcome Blocking:** Websites like Amazon, Zara, and Farfetch use CAPTCHAs and aggressive rate limiting.
2.  **Reliability Cost (Non-Free Requirement):** To ensure the application's scheduled tasks run reliably against these measures, the use of **paid commercial proxy services** is practically mandatory.
    *   **Use Bright Data's Services:** Integrate services like the **Web Unlocker** or **Scraping Browser** to manage IP rotation, solve CAPTCHAs automatically, and execute JavaScript code remotely, ensuring the scraping attempts succeed.
    *   **Use Specialized APIs (Amazon):** For high-volume price tracking on Amazon, the **eCommerce Scraping API** is highly efficient. The AI can send the product's ASIN to the API and receive the parsed price data directly in JSON format.

**Conclusion:** The application can be built using free, open-source code (Python/Django/Beautiful Soup or Next.js/Puppeteer). However, to satisfy the requirement of **reliable, continuous (3 months and all time) checking** against complex sites like Amazon, Zara, and H&M without continuous manual intervention, the application must integrate a paid, third-party proxy solution to avoid being blocked.